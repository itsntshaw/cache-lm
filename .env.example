# Example environment variables (no secrets).
# Copy this file to `.env` and fill in values locally. Never commit `.env`.
# More detail: `ENV.md`.

# OpenAI-compatible endpoint (e.g., vLLM) + model selection
OPENAI_BASE_URL=http://89.169.108.198:30080/v1
OPENAI_MODEL=Qwen/Qwen3-30B-A3B-Instruct-2507
OPENAI_API_KEY=vllm_sk_***
OPENAI_TEMPERATURE=0
OPENAI_MAX_TOKENS=256

# Controls whether expert nodes call the live model.
# Unit tests always force stub mode regardless.
CACHE_LM_MODE=llm

# Router selection:
# - rules: deterministic keyword router (offline)
# - llm: lightweight LLM router (no manual)
# Defaults to: llm when CACHE_LM_MODE=llm, otherwise rules.
# Set explicitly if you want to override defaults:
#   CACHE_LM_ROUTER_MODE=llm
#   CACHE_LM_ROUTER_MODE=rules
CACHE_LM_ROUTER_MODE=llm

# Expert execution strategy:
# - sequential: call experts one-by-one (best for demonstrating cache-warming TTFT)
# - parallel: call selected experts concurrently (requires reducer-safe state updates)
# Defaults to: sequential
CACHE_LM_EXPERT_EXECUTION=sequential

# Optional: enable/disable online integration tests
RUN_LLM_TESTS=1

# Optional: CLI persistence defaults
# These are used by `cache-lm run` when `--checkpoint-db`/`--thread-id` are omitted.
CACHE_LM_CHECKPOINT_DB=.cache_lm/checkpoints.sqlite
CACHE_LM_THREAD_ID=demo
